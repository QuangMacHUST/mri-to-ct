import os
import time
import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import numpy as np
from typing import Dict, Tuple

from cached_data_loader import CachedDataLoaderManager
from optimized_cached_data_loader import OptimizedDataLoaderManager
from volume_based_cached_loader import VolumeCachedDataLoaderManager
from multi_slice_cached_loader import MultiSliceDataLoaderManager
from models import CycleGAN, weights_init_normal
from metrics import MetricsCalculator
from utils import save_checkpoint, load_checkpoint, create_sample_images

class CycleGANTrainer:
    """
    CycleGAN Trainer vá»›i cáº£i tiáº¿n learning rate scheduling vÃ  early stopping
    """
    
    def __init__(self, 
                 config: Dict,
                 device: str = 'cuda',
                 resume_from_checkpoint: bool = True):
        """
        Args:
            config: dictionary chá»©a cÃ¡c tham sá»‘ training
            device: thiáº¿t bá»‹ training (cuda/cpu)
            resume_from_checkpoint: cÃ³ táº£i láº¡i checkpoint khÃ´ng
        """
        self.config = config
        self.device = torch.device(device)
        self.resume_from_checkpoint = resume_from_checkpoint
        
        # Training state
        self.current_epoch = 0
        self.best_ssim = 0.0
        self.epochs_without_improvement = 0
        self.max_patience = 15  # Early stopping patience
        
        # Khá»Ÿi táº¡o model
        self.model = CycleGAN(
            input_nc=config['input_nc'],
            output_nc=config['output_nc'],
            n_residual_blocks=config['n_residual_blocks'],
            discriminator_layers=config['discriminator_layers']
        ).to(self.device)
        
        # Khá»Ÿi táº¡o trá»ng sá»‘
        self.model.apply(weights_init_normal)
        
        # Optimizers vá»›i learning rate tháº¥p hÆ¡n
        self.optimizer_G = optim.Adam(
            list(self.model.G_MRI2CT.parameters()) + list(self.model.G_CT2MRI.parameters()),
            lr=config['lr_G'],
            betas=(config['beta1'], config['beta2'])
        )
        
        self.optimizer_D = optim.Adam(
            list(self.model.D_CT.parameters()) + list(self.model.D_MRI.parameters()),
            lr=config['lr_D'],
            betas=(config['beta1'], config['beta2'])
        )
        
        # Learning rate schedulers - Cosine annealing vá»›i warm restart
        self.scheduler_G = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer_G, 
            T_0=20,      # Restart má»—i 20 epochs
            T_mult=2,    # TÄƒng cycle length x2 má»—i restart
            eta_min=config['lr_G'] * 0.01  # Min LR = 1% cá»§a initial LR
        )
        
        self.scheduler_D = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer_D,
            T_0=20,
            T_mult=2, 
            eta_min=config['lr_D'] * 0.01
        )
        
        # KhÃ´ng cáº§n metrics trackers ná»¯a - tÃ­nh trá»±c tiáº¿p trong má»—i epoch
        
        # Tensorboard writer
        self.writer = SummaryWriter(log_dir=config['log_dir'])
        
        # Thá»­ load checkpoint náº¿u cÃ³
        if self.resume_from_checkpoint:
            self.load_checkpoint()
        
        print(f"Model khá»Ÿi táº¡o thÃ nh cÃ´ng trÃªn {device}")
        print(f"Tá»•ng sá»‘ parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"Báº¯t Ä‘áº§u tá»« epoch: {self.current_epoch}")
        if self.best_ssim > 0:
            print(f"Best SSIM hiá»‡n táº¡i: {self.best_ssim:.4f}")
    
    def load_checkpoint(self):
        """
        Táº£i checkpoint gáº§n nháº¥t Ä‘á»ƒ tiáº¿p tá»¥c training
        """
        checkpoint_dir = self.config['checkpoint_dir']
        
        # TÃ¬m checkpoint gáº§n nháº¥t
        latest_checkpoint = None
        latest_epoch = -1
        
        if os.path.exists(checkpoint_dir):
            for filename in os.listdir(checkpoint_dir):
                if filename.startswith('checkpoint_epoch_') and filename.endswith('.pth'):
                    try:
                        epoch_num = int(filename.split('_')[2].split('.')[0])
                        if epoch_num > latest_epoch:
                            latest_epoch = epoch_num
                            latest_checkpoint = os.path.join(checkpoint_dir, filename)
                    except:
                        continue
        
        # Táº£i checkpoint náº¿u tÃ¬m tháº¥y
        if latest_checkpoint and os.path.exists(latest_checkpoint):
            try:
                print(f"Äang táº£i checkpoint: {latest_checkpoint}")
                checkpoint = torch.load(latest_checkpoint, map_location=self.device, weights_only=False)
                
                # Load model state
                self.model.load_state_dict(checkpoint['model_state_dict'])
                
                # Load optimizer states
                self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
                self.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])
                
                # Load scheduler states
                self.scheduler_G.load_state_dict(checkpoint['scheduler_G_state_dict'])
                self.scheduler_D.load_state_dict(checkpoint['scheduler_D_state_dict'])
                
                # Load training state
                self.current_epoch = checkpoint['epoch'] + 1  # Báº¯t Ä‘áº§u tá»« epoch tiáº¿p theo
                self.best_ssim = checkpoint.get('best_ssim', 0.0)
                
                print(f"âœ… ÄÃ£ táº£i checkpoint thÃ nh cÃ´ng tá»« epoch {checkpoint['epoch']}")
                print(f"   Sáº½ tiáº¿p tá»¥c training tá»« epoch {self.current_epoch}")
                
            except Exception as e:
                print(f"âŒ Lá»—i khi táº£i checkpoint: {e}")
                print("   Sáº½ báº¯t Ä‘áº§u training tá»« Ä‘áº§u")
                self.current_epoch = 0
                self.best_ssim = 0.0
        else:
            print("KhÃ´ng tÃ¬m tháº¥y checkpoint. Báº¯t Ä‘áº§u training tá»« Ä‘áº§u.")
    
    def get_current_lr(self):
        """Láº¥y learning rate hiá»‡n táº¡i"""
        return {
            'lr_G': self.optimizer_G.param_groups[0]['lr'],
            'lr_D': self.optimizer_D.param_groups[0]['lr']
        }
    
    def train_epoch(self, train_loader) -> Dict[str, float]:
        """
        Training má»™t epoch vá»›i GPU memory monitoring, gradient clipping vÃ  learning rate scheduling
        """
        self.model.train()
        
        # Training metrics
        total_g_loss = 0.0
        total_d_loss = 0.0
        total_metrics = {
            'mae': 0.0, 'mse': 0.0, 'rmse': 0.0, 
            'psnr': 0.0, 'ssim': 0.0, 'ncc': 0.0
        }
        
        num_batches = len(train_loader)
        
        # Progress bar vá»›i thÃ´ng tin learning rate
        current_lr = self.get_current_lr()
        pbar = tqdm(train_loader, 
                   desc=f"Training Epoch {self.current_epoch+1}: LR_G={current_lr['lr_G']:.6f}")
        
        for i, batch in enumerate(pbar):
            real_mri = batch['mri'].to(self.device)
            real_ct = batch['ct'].to(self.device)
            
            # Kiá»ƒm tra gradient explosion trÆ°á»›c khi training
            if self._check_gradient_explosion():
                print(f"âš ï¸  PhÃ¡t hiá»‡n gradient explosion táº¡i batch {i}, Ä‘áº·t láº¡i learning rate...")
                self._reset_learning_rate()
                continue
                
            # =============== Train Generator ===============
            self.optimizer_G.zero_grad()
            
            # Generate fake images
            outputs = self.model(real_mri, real_ct)
            
            # Compute generator losses
            g_losses = self.model.generator_loss(real_mri, real_ct, outputs)
            g_losses['total'].backward()
            
            # Gradient clipping cho Generator vá»›i adaptive norm
            grad_norm_g = torch.nn.utils.clip_grad_norm_(
                list(self.model.G_MRI2CT.parameters()) + list(self.model.G_CT2MRI.parameters()),
                max_norm=5.0  # Giáº£m tá»« 10.0 xuá»‘ng 5.0
            )
            
            self.optimizer_G.step()
            
            # =============== Train Discriminator ===============  
            self.optimizer_D.zero_grad()
            
            # Discriminator CT
            fake_ct = outputs['fake_ct'].detach()  # Detach Ä‘á»ƒ khÃ´ng backward qua Generator
            loss_D_CT = self.model.discriminator_loss(real_ct, fake_ct, self.model.D_CT)
            
            # Discriminator MRI  
            fake_mri = outputs['fake_mri'].detach()
            loss_D_MRI = self.model.discriminator_loss(real_mri, fake_mri, self.model.D_MRI)
            
            loss_D_total = (loss_D_CT + loss_D_MRI) * 0.5
            loss_D_total.backward()
            
            # Gradient clipping cho Discriminators
            grad_norm_d = torch.nn.utils.clip_grad_norm_(
                list(self.model.D_CT.parameters()) + list(self.model.D_MRI.parameters()),
                max_norm=5.0  # Giáº£m tá»« 10.0 xuá»‘ng 5.0
            )
            
            self.optimizer_D.step()
            
            # Accumulate losses
            total_g_loss += g_losses['total'].item()
            total_d_loss += loss_D_total.item()
            
            # Compute metrics cho fake_ct
            with torch.no_grad():
                metrics = MetricsCalculator.calculate_all_metrics(outputs['fake_ct'], real_ct)
                # Key mapping tá»« metrics calculator sang total_metrics
                metric_mapping = {
                    'mae': 'MAE',
                    'mse': 'MSE', 
                    'rmse': 'RMSE',
                    'psnr': 'PSNR',
                    'ssim': 'SSIM',
                    'ncc': 'NCC'
                }
                for key in total_metrics:
                    if metric_mapping[key] in metrics:
                        total_metrics[key] += metrics[metric_mapping[key]]
            
            # Update progress bar vá»›i gradient norms
            pbar.set_postfix({
                'G_loss': g_losses['total'].item(),
                'D_loss': loss_D_total.item(),
                'SSIM': metrics.get('SSIM', 0.0),
                'Grad_G': f"{grad_norm_g:.2f}",
                'Grad_D': f"{grad_norm_d:.2f}"
            })
            
            # Kiá»ƒm tra memory leak
            if i % 50 == 0:
                torch.cuda.empty_cache()
        
        # Step learning rate schedulers
        self.scheduler_G.step()
        self.scheduler_D.step()
        
        # Average losses vÃ  metrics
        avg_g_loss = total_g_loss / num_batches
        avg_d_loss = total_d_loss / num_batches
        
        avg_metrics = {}
        for key in total_metrics:
            avg_metrics[key] = total_metrics[key] / num_batches
        
        return {
            'g_loss': avg_g_loss,
            'd_loss': avg_d_loss,
            **avg_metrics
        }
    
    def _check_gradient_explosion(self) -> bool:
        """Kiá»ƒm tra gradient explosion"""
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.data.norm(2)
                if grad_norm > 100.0:  # Threshold cho gradient explosion
                    return True
        return False
    
    def _reset_learning_rate(self):
        """Äáº·t láº¡i learning rate khi gáº·p gradient explosion"""
        for g in self.optimizer_G.param_groups:
            g['lr'] *= 0.5  # Giáº£m learning rate má»™t ná»­a
        for g in self.optimizer_D.param_groups:
            g['lr'] *= 0.5
    
    def validate_epoch(self, val_loader) -> Dict[str, float]:
        """
        Validation má»™t epoch
        """
        self.model.eval()
        
        # Validation metrics
        total_g_loss = 0.0
        total_d_loss = 0.0
        total_metrics = {
            'mae': 0.0, 'mse': 0.0, 'rmse': 0.0, 
            'psnr': 0.0, 'ssim': 0.0, 'ncc': 0.0
        }
        
        num_batches = len(val_loader)
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                real_mri = batch['mri'].to(self.device)
                real_ct = batch['ct'].to(self.device)
                
                # Forward pass
                outputs = self.model(real_mri, real_ct)
                
                # Generator loss (khÃ´ng cáº§n discriminator loss trong validation)
                g_losses = self.model.generator_loss(real_mri, real_ct, outputs)
                
                # Accumulate losses
                total_g_loss += g_losses['total'].item()
                
                # Compute metrics cho fake_ct
                metrics = MetricsCalculator.calculate_all_metrics(outputs['fake_ct'], real_ct)
                # Key mapping tá»« metrics calculator sang total_metrics
                metric_mapping = {
                    'mae': 'MAE',
                    'mse': 'MSE', 
                    'rmse': 'RMSE',
                    'psnr': 'PSNR',
                    'ssim': 'SSIM',
                    'ncc': 'NCC'
                }
                for key in total_metrics:
                    if metric_mapping[key] in metrics:
                        total_metrics[key] += metrics[metric_mapping[key]]
        
        # Average losses vÃ  metrics
        avg_g_loss = total_g_loss / num_batches
        avg_d_loss = 0.0  # KhÃ´ng cÃ³ D loss trong validation
        
        avg_metrics = {}
        for key in total_metrics:
            avg_metrics[key] = total_metrics[key] / num_batches
        
        return {
            'g_loss': avg_g_loss,
            'd_loss': avg_d_loss,
            **avg_metrics
        }
    
    def log_to_tensorboard(self, 
                          train_losses: Dict[str, float],
                          val_losses: Dict[str, float]):
        """
        Ghi log vÃ o tensorboard
        """
        # Training losses
        self.writer.add_scalar('Loss/Train_G', train_losses['g_loss'], self.current_epoch)
        self.writer.add_scalar('Loss/Train_D', train_losses['d_loss'], self.current_epoch)
        
        # Validation losses
        self.writer.add_scalar('Loss/Val_G', val_losses['g_loss'], self.current_epoch)
        self.writer.add_scalar('Loss/Val_D', val_losses['d_loss'], self.current_epoch)
        
        # Training metrics
        for metric_name in ['mae', 'mse', 'rmse', 'psnr', 'ssim', 'ncc']:
            if metric_name in train_losses:
                self.writer.add_scalar(f'Metrics/Train_{metric_name.upper()}', train_losses[metric_name], self.current_epoch)
        
        # Validation metrics
        for metric_name in ['mae', 'mse', 'rmse', 'psnr', 'ssim', 'ncc']:
            if metric_name in val_losses:
                self.writer.add_scalar(f'Metrics/Val_{metric_name.upper()}', val_losses[metric_name], self.current_epoch)
    
    def save_model(self, is_best: bool = False):
        """
        LÆ°u model checkpoint
        """
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_G_state_dict': self.optimizer_G.state_dict(),
            'optimizer_D_state_dict': self.optimizer_D.state_dict(),
            'scheduler_G_state_dict': self.scheduler_G.state_dict(),
            'scheduler_D_state_dict': self.scheduler_D.state_dict(),
            'best_ssim': self.best_ssim,
            'config': self.config
        }
        
        # LÆ°u checkpoint thÆ°á»ng xuyÃªn
        save_path = os.path.join(self.config['checkpoint_dir'], f'checkpoint_epoch_{self.current_epoch}.pth')
        torch.save(checkpoint, save_path)
        
        # LÆ°u best model
        if is_best:
            best_path = os.path.join(self.config['checkpoint_dir'], 'best_model.pth')
            torch.save(checkpoint, best_path)
            print(f"ÄÃ£ lÆ°u best model vá»›i SSIM: {self.best_ssim:.4f}")
    
    def train(self, train_loader, val_loader):
        """
        Main training loop vá»›i early stopping vÃ  learning rate monitoring
        """
        print(f"ğŸš€ Báº¯t Ä‘áº§u training tá»« epoch {self.current_epoch + 1}")
        print(f"ğŸ“Š Initial learning rates - G: {self.config['lr_G']:.6f}, D: {self.config['lr_D']:.6f}")
        
        start_time = time.time()
        
        for epoch in range(self.current_epoch, self.config['num_epochs']):
            self.current_epoch = epoch
            epoch_start_time = time.time()
            
            # Training
            train_losses = self.train_epoch(train_loader)
            
            # Validation
            val_losses = self.validate_epoch(val_loader)
            
            # Log current learning rates
            current_lr = self.get_current_lr()
            
            # Print epoch results vá»›i learning rate info
            epoch_time = time.time() - epoch_start_time
            print(f"\nEpoch {epoch+1}/{self.config['num_epochs']} - Time: {epoch_time:.2f}s")
            print(f"LR_G: {current_lr['lr_G']:.6f} | LR_D: {current_lr['lr_D']:.6f}")
            print(f"Train Loss: {train_losses['g_loss'] + train_losses['d_loss']:.4f} | Val Loss: {val_losses['g_loss'] + val_losses['d_loss']:.4f}")
            
            # Print detailed metrics
            print("\nTrain Metrics:")
            print("-" * 50)
            print(f"MAE: {train_losses['mae']:.6f}")
            print(f"MSE: {train_losses['mse']:.6f}") 
            print(f"RMSE: {train_losses['rmse']:.6f}")
            print(f"PSNR: {train_losses['psnr']:.4f} dB")
            print(f"SSIM: {train_losses['ssim']:.4f}")
            print(f"NCC: {train_losses['ncc']:.4f}")
            print("-" * 50)
            
            print("\nValidation Metrics:")
            print("-" * 50)
            print(f"MAE: {val_losses['mae']:.6f}")
            print(f"MSE: {val_losses['mse']:.6f}")
            print(f"RMSE: {val_losses['rmse']:.6f}")
            print(f"PSNR: {val_losses['psnr']:.4f} dB")
            print(f"SSIM: {val_losses['ssim']:.4f}")
            print(f"NCC: {val_losses['ncc']:.4f}")
            print("-" * 50)
            
            # Log to tensorboard vá»›i learning rates
            self.log_to_tensorboard(train_losses, val_losses)
            self.writer.add_scalar('Learning_Rate/Generator', current_lr['lr_G'], epoch)
            self.writer.add_scalar('Learning_Rate/Discriminator', current_lr['lr_D'], epoch)
            
            # Early stopping based on SSIM
            current_ssim = val_losses['ssim']
            
            # Kiá»ƒm tra cáº£i thiá»‡n
            if current_ssim > self.best_ssim:
                self.best_ssim = current_ssim
                self.epochs_without_improvement = 0
                self.save_model(is_best=True)
                print(f"ğŸ‰ New best SSIM: {self.best_ssim:.4f}")
            else:
                self.epochs_without_improvement += 1
                print(f"â³ Epochs without improvement: {self.epochs_without_improvement}/{self.max_patience}")
            
            # Kiá»ƒm tra SSIM collapse (giáº£m quÃ¡ nhanh)
            if epoch > 50 and current_ssim < 0.1:  # SSIM quÃ¡ tháº¥p
                print(f"âš ï¸  SSIM collapse detected! Current: {current_ssim:.4f}")
                print("ğŸ”„ Reducing learning rate dramatically...")
                
                # Giáº£m learning rate ráº¥t máº¡nh
                for g in self.optimizer_G.param_groups:
                    g['lr'] *= 0.1
                for g in self.optimizer_D.param_groups:
                    g['lr'] *= 0.1
                
                self.epochs_without_improvement = 0  # Reset patience
            
            # Early stopping
            if self.epochs_without_improvement >= self.max_patience:
                print(f"\nğŸ›‘ Early stopping triggered after {self.epochs_without_improvement} epochs without improvement")
                print(f"ğŸ† Best SSIM achieved: {self.best_ssim:.4f}")
                break
            
            # Save checkpoint
            if (epoch + 1) % self.config['save_freq'] == 0:
                self.save_model()
            
            # Táº¡o sample images
            if (epoch + 1) % self.config['sample_freq'] == 0:
                self.create_sample_images(val_loader)
        
        total_time = time.time() - start_time
        print(f"\nTraining hoÃ n thÃ nh! Tá»•ng thá»i gian: {total_time/3600:.2f} giá»")
        print(f"Best SSIM: {self.best_ssim:.4f}")
        
        self.writer.close()
    
    def create_sample_images(self, val_loader):
        """
        Táº¡o áº£nh máº«u Ä‘á»ƒ kiá»ƒm tra káº¿t quáº£
        """
        self.model.eval()
        
        with torch.no_grad():
            # Láº¥y batch Ä‘áº§u tiÃªn tá»« validation set
            batch = next(iter(val_loader))
            real_mri = batch['mri'][:4].to(self.device)  # Láº¥y 4 sample Ä‘áº§u
            real_ct = batch['ct'][:4].to(self.device)
            
            # Generate fake images
            fake_ct = self.model.G_MRI2CT(real_mri)
            fake_mri = self.model.G_CT2MRI(real_ct)
            
            # Cycle consistency
            rec_mri = self.model.G_CT2MRI(fake_ct)
            rec_ct = self.model.G_MRI2CT(fake_mri)
            
            # Save sample images
            save_dir = os.path.join(self.config['sample_dir'], f'epoch_{self.current_epoch}')
            os.makedirs(save_dir, exist_ok=True)
            
            # LÆ°u tá»«ng loáº¡i áº£nh
            self._save_image_batch(real_mri, os.path.join(save_dir, 'real_mri.png'))
            self._save_image_batch(real_ct, os.path.join(save_dir, 'real_ct.png'))
            self._save_image_batch(fake_ct, os.path.join(save_dir, 'fake_ct.png'))
            self._save_image_batch(fake_mri, os.path.join(save_dir, 'fake_mri.png'))
            self._save_image_batch(rec_mri, os.path.join(save_dir, 'rec_mri.png'))
            self._save_image_batch(rec_ct, os.path.join(save_dir, 'rec_ct.png'))
    
    def _save_image_batch(self, images, save_path):
        """
        LÆ°u batch áº£nh thÃ nh file
        """
        import matplotlib.pyplot as plt
        
        # Chuyá»ƒn vá» numpy
        images_np = images.detach().cpu().numpy()
        batch_size = images_np.shape[0]
        
        # Táº¡o subplot dá»±a trÃªn batch size thá»±c táº¿
        fig, axes = plt.subplots(1, min(4, batch_size), figsize=(16, 4))
        
        # Náº¿u chá»‰ cÃ³ 1 image, axes khÃ´ng pháº£i lÃ  array
        if batch_size == 1:
            axes = [axes]
        
        for i in range(min(4, batch_size)):
            img = images_np[i, 0]  # Láº¥y channel Ä‘áº§u tiÃªn
            axes[i].imshow(img, cmap='gray')
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()


def main():
    """
    HÃ m main Ä‘á»ƒ cháº¡y training
    """
    # Cáº¥u hÃ¬nh training tá»‘i Æ°u cho GTX 1650 (4GB VRAM) vá»›i learning rate tháº¥p hÆ¡n
    config = {
        # Data parameters  
        'cache_dir': '../preprocessed_cache',  # Sá»­ dá»¥ng cached data
        'batch_size': 3,          # Batch size nhá»
        'num_workers': 2,         # TÄƒng workers vÃ¬ chá»‰ load cache
        'train_split': 0.8,
        'augmentation_prob': 0.8, # XÃ¡c suáº¥t augmentation
        
        # Model parameters - Tá»‘i Æ°u vá»›i cached data
        'input_nc': 1,
        'output_nc': 1,
        'n_residual_blocks': 9,   
        'discriminator_layers': 3,
        
        # Training parameters - Learning rate tháº¥p hÆ¡n Ä‘á»ƒ trÃ¡nh collapse
        'num_epochs': 200,        
        'lr_G': 0.00005,          # Giáº£m tá»« 0.0002 xuá»‘ng 0.00005
        'lr_D': 0.00005,          # Giáº£m tá»« 0.0002 xuá»‘ng 0.00005  
        'beta1': 0.5,
        'beta2': 0.999,
        'decay_epoch': 50,        # Decay sá»›m hÆ¡n
        'decay_epochs': 50,
        
        # Directories
        'checkpoint_dir': 'checkpoints',
        'log_dir': 'logs',
        'sample_dir': 'samples',
        
        # Save frequencies - Vá»›i cached data training nhanh hÆ¡n
        'save_freq': 2,           # Save má»—i 5 epochs
        'sample_freq': 2          # Sample má»—i 5 epochs
    }
    
    # Táº¡o thÆ° má»¥c cáº§n thiáº¿t
    for dir_name in ['checkpoint_dir', 'log_dir', 'sample_dir']:
        os.makedirs(config[dir_name], exist_ok=True)
    
    # Kiá»ƒm tra GPU
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Sá»­ dá»¥ng device: {device}")
    
    # Há»i user cÃ³ muá»‘n resume training khÃ´ng
    resume_training = True
    if os.path.exists(config['checkpoint_dir']):
        # Kiá»ƒm tra xem cÃ³ checkpoint nÃ o khÃ´ng
        checkpoint_files = [f for f in os.listdir(config['checkpoint_dir']) 
                          if f.startswith('checkpoint_epoch_') and f.endswith('.pth')]
        
        if checkpoint_files:
            # Sáº¯p xáº¿p checkpoint files theo epoch number
            def extract_epoch(filename):
                try:
                    return int(filename.split('_epoch_')[1].split('.pth')[0])
                except:
                    return -1
            
            sorted_checkpoints = sorted(checkpoint_files, key=extract_epoch)
            latest_checkpoint_file = sorted_checkpoints[-1]
            latest_epoch = extract_epoch(latest_checkpoint_file)
            
            print(f"\nğŸ” TÃ¬m tháº¥y {len(checkpoint_files)} checkpoint trong thÆ° má»¥c:")
            # Hiá»ƒn thá»‹ 3 checkpoint gáº§n nháº¥t vá»›i epoch numbers
            for f in sorted_checkpoints[-3:]:
                epoch_num = extract_epoch(f)
                print(f"   - {f} (epoch {epoch_num})")
            
            print(f"\nğŸ¯ Checkpoint gáº§n nháº¥t: {latest_checkpoint_file} (epoch {latest_epoch})")
            
            choice = input("\nâ“ Báº¡n cÃ³ muá»‘n tiáº¿p tá»¥c training tá»« checkpoint gáº§n nháº¥t? (y/n): ").lower().strip()
            resume_training = choice in ['y', 'yes', '1', 'true', '']
            
            if not resume_training:
                print("âš ï¸  Sáº½ báº¯t Ä‘áº§u training tá»« Ä‘áº§u (checkpoint cÅ© sáº½ khÃ´ng bá»‹ xÃ³a)")
    
    # Kiá»ƒm tra cache tá»“n táº¡i
    if not os.path.exists(config['cache_dir']):
        print(f"âŒ Cache directory not found: {config['cache_dir']}")
        print("   Run: python preprocess_and_cache.py first!")
        return
    
    # Há»i user chá»n loading strategy
    print("\nğŸ¤” Chá»n data loading strategy:")
    print("   1. VOLUME-BASED (original): 42 samples/epoch, ~1.5s/epoch (giá»‘ng data_loader.py cÅ©)")
    print("   2. MULTI-SLICE (recommended): 42Ã—N slices/epoch, tÄƒng data diversity!")
    print("   3. SLICE-BASED optimized: ~1,260 slices/epoch, ~1.4 phÃºt/epoch")
    print("   4. SLICE-BASED full: ~4,681 slices/epoch, ~5.4 phÃºt/epoch")
    
    choice = input("â“ Chá»n strategy (1/2/3/4): ").strip()
    
    if choice == "1" or choice == "":
        loading_strategy = "volume"
    elif choice == "2":
        loading_strategy = "multi_slice"
    elif choice == "3":
        loading_strategy = "slice_optimized"
    elif choice == "4":
        loading_strategy = "slice_full"
    else:
        print("Invalid choice, using volume-based (default)")
        loading_strategy = "volume"
    
        # Táº¡o cached data loaders theo strategy Ä‘Ã£ chá»n
    if loading_strategy == "volume":
        print("\nğŸš€ Äang táº¡o VOLUME-BASED cached data loaders...")
        loader_manager = VolumeCachedDataLoaderManager(config['cache_dir'])
        
        train_loader, val_loader = loader_manager.create_train_val_loaders(
            batch_size=config['batch_size'],
            train_split=config['train_split'],
            num_workers=config['num_workers'],
            augmentation_prob=config['augmentation_prob']
        )
        
    elif loading_strategy == "multi_slice":
        # Há»i sá»‘ slices per patient theo analysis cho SSIM 90%
        print("\nğŸ¯ Chá»n sá»‘ slices per patient (Based on SSIM Analysis):")
        print("   10: Baseline (330 samples/epoch) â†’ Expected SSIM 0.68")
        print("   20: Phase 1 (660 samples/epoch) â†’ Expected SSIM 0.75")  
        print("   50: Phase 2 (1650 samples/epoch) â†’ Expected SSIM 0.87")
        print("   80: Phase 3 (2640 samples/epoch) â†’ Expected SSIM 0.92+ â­")
        print("   100: Maximum (3300 samples/epoch) â†’ Expected SSIM 0.95")
        
        slice_choice = input("â“ Chá»n sá»‘ slices (10/20/50/80/100): ").strip()
        
        if slice_choice == "10":
            slices_per_patient = 10
        elif slice_choice == "20":
            slices_per_patient = 20
        elif slice_choice == "50":
            slices_per_patient = 50
        elif slice_choice == "80":
            slices_per_patient = 80
        elif slice_choice == "100":
            slices_per_patient = 100
        else:
            slices_per_patient = 20  # Default to Phase 1
        
        # Äiá»u chá»‰nh learning rate theo slice count
        original_lr_g = config['lr_G']
        original_lr_d = config['lr_D']
        
        if slices_per_patient >= 80:
            # Very high data - cáº§n LR ráº¥t tháº¥p
            config['lr_G'] = 0.000025
            config['lr_D'] = 0.000025
            print(f"   ğŸ”§ Adjusted LR to {config['lr_G']} (Very High Data)")
        elif slices_per_patient >= 50:
            # High data - LR tháº¥p
            config['lr_G'] = 0.00003
            config['lr_D'] = 0.00003
            print(f"   ğŸ”§ Adjusted LR to {config['lr_G']} (High Data)")
        elif slices_per_patient >= 20:
            # Moderate data - LR moderate
            config['lr_G'] = 0.00004
            config['lr_D'] = 0.00004
            print(f"   ğŸ”§ Adjusted LR to {config['lr_G']} (Moderate Data)")
        else:
            # Low data - keep default
            print(f"   ğŸ”§ Using default LR {config['lr_G']} (Low Data)")
        
        # Äiá»u chá»‰nh batch size cho high slice counts
        original_batch_size = config['batch_size']
        if slices_per_patient >= 50:
            # Giáº£m batch size Ä‘á»ƒ fit memory vá»›i nhiá»u data
            config['batch_size'] = 2
            print(f"   ğŸ”§ Adjusted batch size to {config['batch_size']} (High Data Volume)")
        
        print(f"\nğŸš€ Äang táº¡o MULTI-SLICE cached data loaders vá»›i {slices_per_patient} slices/patient...")
        loader_manager = MultiSliceDataLoaderManager(config['cache_dir'])
        
        # Hiá»ƒn thá»‹ statistics
        stats = loader_manager.get_data_statistics(slices_per_patient)
        print(f"\nğŸ“Š Data Statistics:")
        print(f"   Samples per epoch: {stats['samples_per_epoch']}")
        print(f"   Data utilization: {stats['data_utilization_percent']:.1f}%")
        print(f"   Improvement vs volume-based: {stats['improvement_vs_volume']}")
        
        train_loader, val_loader = loader_manager.create_train_val_loaders(
            batch_size=config['batch_size'],
            train_split=config['train_split'],
            num_workers=config['num_workers'],
            slices_per_patient=slices_per_patient,
            augmentation_prob=config['augmentation_prob']
        )
        
        print(f"âœ… Multi-slice loaders created!")
        print(f"   Training batches/epoch: {len(train_loader)}")
        print(f"   Validation batches: {len(val_loader)}")
        print(f"   Estimated time/epoch: ~{len(train_loader)*5/60:.1f} minutes")
        
        if slices_per_patient >= 80:
            print(f"\nğŸ¯ TARGET: SSIM 90%+ vá»›i {slices_per_patient} slices!")
            print(f"   âš ï¸  High training time but expected breakthrough performance")
            print(f"   ğŸ“ˆ Progressive strategy recommended if first time")
        
    elif loading_strategy == "slice_optimized":
        print("\nğŸš€ Äang táº¡o OPTIMIZED slice-based data loaders...")
        loader_manager = OptimizedDataLoaderManager(config['cache_dir'])
        
        train_loader, val_loader = loader_manager.create_fast_train_val_loaders(
            batch_size=config['batch_size'],
            train_split=config['train_split'],
            num_workers=config['num_workers'],
            slice_sampling_strategy="middle_range",  # Láº¥y 60% slices á»Ÿ giá»¯a
            max_slices_per_patient=30,              # Tá»‘i Ä‘a 30 slices/bá»‡nh nhÃ¢n
            augmentation_prob=config['augmentation_prob']
        )
        
    else:  # slice_full
        print("\nğŸ“Š Äang táº¡o FULL slice-based cached data loaders...")
        loader_manager = CachedDataLoaderManager(config['cache_dir'])
        
        # In thÃ´ng tin cache cho standard loader
        cache_info = loader_manager.get_cache_info()
        print(f"ğŸ“¦ Cache info:")
        print(f"   Total patients: {cache_info['total_patients']}")
        print(f"   Total slices: {cache_info['total_slices']}")
        print(f"   Cache size: {cache_info['cache_size_mb']:.1f} MB")
        print(f"   Save format: {cache_info['save_format']}")
        print(f"ğŸš€ Training sáº½ nhanh hÆ¡n ~450x so vá»›i preprocessing realtime!")
        
        train_loader, val_loader = loader_manager.create_train_val_loaders(
            batch_size=config['batch_size'],
            train_split=config['train_split'],
            num_workers=config['num_workers'],
            augmentation_prob=config['augmentation_prob']
        )
    
    # Khá»Ÿi táº¡o trainer
    trainer = CycleGANTrainer(config, device, resume_from_checkpoint=resume_training)
    
    # CRITICAL FIX: Reset learning rates náº¿u Ä‘Ã£ adjust cho multi-slice
    if loading_strategy == "multi_slice" and resume_training:
        # Äáº·t láº¡i learning rate sau khi load checkpoint Ä‘á»ƒ match config
        current_lr_g = config['lr_G']
        current_lr_d = config['lr_D']
        
        for param_group in trainer.optimizer_G.param_groups:
            param_group['lr'] = current_lr_g
        for param_group in trainer.optimizer_D.param_groups:
            param_group['lr'] = current_lr_d
            
        print(f"ğŸ”§ Reset learning rates after checkpoint loading:")
        print(f"   LR_G: {current_lr_g}")
        print(f"   LR_D: {current_lr_d}")
    
    # Hiá»ƒn thá»‹ initial learning rates
    initial_lr = trainer.get_current_lr()
    print(f"ğŸ“Š Initial learning rates - G: {initial_lr['lr_G']:.6f}, D: {initial_lr['lr_D']:.6f}")
    
    # Báº¯t Ä‘áº§u training vá»›i cached data
    if loading_strategy == "volume":
        print(f"\nğŸš€ Báº¯t Ä‘áº§u VOLUME-BASED training...")
        print(f"   - Giá»‘ng há»‡t data_loader.py cÅ©: {len(train_loader.dataset)} samples/epoch")
        print(f"   - NhÆ°ng preprocessing Ä‘Ã£ cache sáºµn â†’ nhanh hÆ¡n ~4,784x!")
        print(f"   - Random slice selection má»—i epoch")
        print(f"   - Estimated training time: ~{len(train_loader) * 0.167:.1f}s/epoch")
    else:
        print(f"\nğŸš€ Báº¯t Ä‘áº§u SLICE-BASED training vá»›i cached data...")
        print(f"   - {len(train_loader.dataset)} samples/epoch")
        print(f"   - Preprocessing Ä‘Ã£ Ä‘Æ°á»£c cache trÆ°á»›c")
        print(f"   - Má»—i epoch chá»‰ cáº§n load cache + augmentation")
    
    trainer.train(train_loader, val_loader)


if __name__ == "__main__":
    main() 