# D·ª± √°n Chuy·ªÉn ƒë·ªïi MRI th√†nh CT m√¥ ph·ªèng s·ª≠ d·ª•ng CycleGAN - T√†i li·ªáu Chi ti·∫øt

## üìã T·ªïng quan

D·ª± √°n n√†y tri·ªÉn khai m√¥ h√¨nh **CycleGAN** ƒë·ªÉ chuy·ªÉn ƒë·ªïi ·∫£nh MRI th√†nh ·∫£nh CT m√¥ ph·ªèng ph·ª•c v·ª• cho **l·∫≠p k·∫ø ho·∫°ch x·∫° tr·ªã** trong y h·ªçc. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c CycleGAN v·ªõi PatchGAN discriminator v√† t√≠ch h·ª£p nhi·ªÅu loss function ti√™n ti·∫øn bao g·ªìm perceptual loss, adversarial loss, cycle consistency loss v√† identity loss.

### üéØ M·ª•c ti√™u d·ª± √°n
- **V·∫•n ƒë·ªÅ gi·∫£i quy·∫øt**: T·∫°o ra ·∫£nh CT m√¥ ph·ªèng t·ª´ ·∫£nh MRI ƒë·ªÉ h·ªó tr·ª£ l·∫≠p k·∫ø ho·∫°ch x·∫° tr·ªã khi kh√¥ng c√≥ s·∫µn ·∫£nh CT
- **√ù nghƒ©a y h·ªçc**: Gi·∫£m li·ªÅu ph√≥ng x·∫° cho b·ªánh nh√¢n, ti·∫øt ki·ªám chi ph√≠ v√† th·ªùi gian
- **Th√°ch th·ª©c k·ªπ thu·∫≠t**: ƒê·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c cao c·ªßa ·∫£nh CT m√¥ ph·ªèng ƒë·ªÉ ph·ª•c v·ª• m·ª•c ƒë√≠ch y t·∫ø

## üèóÔ∏è C·∫•u tr√∫c d·ª± √°n

```
mri-to-ct/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ MRI/                 # D·ªØ li·ªáu MRI training (brain_001.nii.gz - brain_046.nii.gz)
‚îÇ   ‚îú‚îÄ‚îÄ CT/                  # D·ªØ li·ªáu CT training (brain_001.nii.gz - brain_046.nii.gz)
‚îÇ   ‚îî‚îÄ‚îÄ Test/
‚îÇ       ‚îú‚îÄ‚îÄ MRI/            # D·ªØ li·ªáu MRI test
‚îÇ       ‚îî‚îÄ‚îÄ CT/             # D·ªØ li·ªáu CT test
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py      # T·∫£i v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ models.py           # Ki·∫øn tr√∫c CycleGAN v√† c√°c m√¥ h√¨nh
‚îÇ   ‚îú‚îÄ‚îÄ train.py            # Script training
‚îÇ   ‚îú‚îÄ‚îÄ test.py             # Script testing v√† ƒë√°nh gi√°
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py          # C√°c metrics ƒë√°nh gi√° (MAE, MSE, SSIM, PSNR)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py            # C√°c h√†m ti·ªán √≠ch
‚îú‚îÄ‚îÄ requirements.txt        # C√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
‚îî‚îÄ‚îÄ README.md
```

## üî¨ Chi ti·∫øt Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu

### 1. N4 Bias Field Correction

**M·ª•c ƒë√≠ch**: Hi·ªáu ch·ªânh ƒë·ªô l·ªách t·ª´ tr∆∞·ªùng trong ·∫£nh MRI

**L√Ω do s·ª≠ d·ª•ng**:
- ·∫¢nh MRI th∆∞·ªùng b·ªã nhi·ªÖu do s·ª± kh√¥ng ƒë·ªìng nh·∫•t c·ªßa t·ª´ tr∆∞·ªùng
- Bias field g√¢y ra ƒë·ªô s√°ng kh√¥ng ƒë·ªÅu tr√™n to√†n b·ªô ·∫£nh
- ·∫¢nh h∆∞·ªüng ƒë·∫øn ch·∫•t l∆∞·ª£ng training v√† k·∫øt qu·∫£ cu·ªëi c√πng

**Tham s·ªë ƒë∆∞·ª£c s·ª≠ d·ª•ng**:
```python
corrector = sitk.N4BiasFieldCorrectionImageFilter()
corrector.SetMaximumNumberOfIterations([50] * 4)
```
- **50 iterations √ó 4 levels**: ƒê·∫£m b·∫£o hi·ªáu ch·ªânh ch√≠nh x√°c qua nhi·ªÅu level resolution
- **4 levels**: Multi-resolution approach gi√∫p x·ª≠ l√Ω bias field ·ªü nhi·ªÅu t·∫ßn s·ªë kh√°c nhau

**T√°c ƒë·ªông**: C·∫£i thi·ªán contrast v√† ƒë·ªìng nh·∫•t intensity, t·∫°o ƒëi·ªÅu ki·ªán t·ªët cho c√°c b∆∞·ªõc x·ª≠ l√Ω ti·∫øp theo

### 2. Binary Masking v·ªõi Otsu Thresholding

**M·ª•c ƒë√≠ch**: T·∫°o mask ƒë·ªÉ ph√¢n t√°ch v√πng n√£o kh·ªèi background

**L√Ω do s·ª≠ d·ª•ng**:
- Lo·∫°i b·ªè noise v√† artifacts ·ªü v√πng background
- T·∫≠p trung training v√†o v√πng quan tr·ªçng (m√¥ n√£o)
- Gi·∫£m thi·ªÉu ·∫£nh h∆∞·ªüng c·ªßa v√πng kh√¥ng ch·ª©a th√¥ng tin y h·ªçc

**Quy tr√¨nh x·ª≠ l√Ω**:
```python
# Chu·∫©n h√≥a v·ªÅ [0, 255]
normalized = ((image_array - image_array.min()) / 
             (image_array.max() - image_array.min()) * 255).astype(np.uint8)

# Otsu thresholding
threshold = filters.threshold_otsu(normalized)
binary_mask = normalized > threshold

# Post-processing
binary_mask = morphology.remove_small_objects(binary_mask, min_size=1000)
binary_mask = ndimage.binary_fill_holes(binary_mask)
```

**Tham s·ªë chi ti·∫øt**:
- **min_size=1000**: Lo·∫°i b·ªè c√°c object nh·ªè h∆°n 1000 pixels, gi·ªØ l·∫°i ch·ªâ c√°c v√πng m√¥ n√£o ch√≠nh
- **binary_fill_holes**: L·∫•p ƒë·∫ßy c√°c l·ªó h·ªïng nh·ªè trong mask, t·∫°o v√πng li√™n t·ª•c

**Hi·ªáu qu·∫£**: C·∫£i thi·ªán SNR (Signal-to-Noise Ratio) v√† gi·∫£m computational cost

### 3. Intensity Normalization

**M·ª•c ƒë√≠ch**: Chu·∫©n h√≥a gi√° tr·ªã pixel ƒë·ªÉ ·ªïn ƒë·ªãnh training

**Ph∆∞∆°ng ph√°p Min-Max normalization**:
```python
masked_values = image_array[mask > 0]
min_val = np.min(masked_values)
max_val = np.max(masked_values)
if max_val > min_val:
    # Min-Max normalization v·ªÅ [0, 1]
    image_array = (image_array - min_val) / (max_val - min_val)

# √Åp d·ª•ng mask
image_array = image_array * mask

# Chuy·ªÉn v·ªÅ [-1, 1] ƒë·ªÉ ph√π h·ª£p v·ªõi Tanh activation
image_array = image_array * 2.0 - 1.0
```

**L√Ω do ch·ªçn Min-Max**:
- **Full dynamic range utilization**: S·ª≠ d·ª•ng to√†n b·ªô range [0,1] sau khi ƒë√£ lo·∫°i b·ªè outliers b·∫±ng mask
- **Intuitive interpretation**: 0 = pixel t·ªëi nh·∫•t trong brain, 1 = pixel s√°ng nh·∫•t trong brain
- **Optimal for generation**: Ph√π h·ª£p v·ªõi generative models khi ƒë√£ c√≥ binary mask lo·∫°i b·ªè background
- **Cross-subject consistency**: C√πng tissue type s·∫Ω c√≥ similar relative position trong [0,1] range

**Chuy·ªÉn ƒë·ªïi [-1, 1]**:
- Ph√π h·ª£p v·ªõi Tanh activation c·ªßa Generator output
- Standard practice trong CycleGAN v√† c√°c generative models
- ƒê·∫£m b·∫£o symmetric range around zero

### 4. Data Augmentation

**M·ª•c ƒë√≠ch**: TƒÉng c∆∞·ªùng d·ªØ li·ªáu ƒë·ªÉ tr√°nh overfitting

**C√°c k·ªπ thu·∫≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng**:

#### 4.1 Geometric Transformations
```python
# Random rotation (-10¬∞ ƒë·∫øn +10¬∞)
if random.random() > 0.5:
    angle = random.uniform(-10, 10)
    mri_slice = ndimage.rotate(mri_slice, angle, reshape=False, mode='nearest')
```
- **G√≥c xoay [-10¬∞, +10¬∞]**: M√¥ ph·ªèng s·ª± thay ƒë·ªïi t∆∞ th·∫ø ƒë·∫ßu t·ª± nhi√™n
- **reshape=False**: Gi·ªØ nguy√™n k√≠ch th∆∞·ªõc ·∫£nh
- **mode='nearest'**: Tr√°nh interpolation artifacts

#### 4.2 Flip Operations
```python
# Horizontal v√† Vertical flip v·ªõi x√°c su·∫•t 50%
if random.random() > 0.5:
    mri_slice = np.fliplr(mri_slice)  # Left-Right flip
if random.random() > 0.5:
    mri_slice = np.flipud(mri_slice)  # Up-Down flip
```
- **X√°c su·∫•t 50%**: C√¢n b·∫±ng gi·ªØa d·ªØ li·ªáu g·ªëc v√† augmented
- **Anatomical consideration**: Ph·∫£n √°nh t√≠nh ƒë·ªëi x·ª©ng t·ª± nhi√™n c·ªßa n√£o

#### 4.3 Intensity Scaling
```python
# Random intensity scaling (¬±10%)
if random.random() > 0.5:
    scale_factor = random.uniform(0.9, 1.1)
    mri_slice = mri_slice * scale_factor
```
- **Scaling range [0.9, 1.1]**: M√¥ ph·ªèng s·ª± bi·∫øn ƒë·ªïi contrast t·ª± nhi√™n
- **Ch·ªâ √°p d·ª•ng cho MRI**: CT c√≥ range c·ªë ƒë·ªãnh n√™n kh√¥ng n√™n thay ƒë·ªïi

**T√°c ƒë·ªông t·ªïng th·ªÉ**: TƒÉng g·∫•p 8 l·∫ßn s·ªë l∆∞·ª£ng d·ªØ li·ªáu hi·ªáu qu·∫£, c·∫£i thi·ªán kh·∫£ nƒÉng generalization

## üß† Ki·∫øn tr√∫c M√¥ h√¨nh Chi ti·∫øt

### Generator Architecture

**Thi·∫øt k·∫ø Encoder-Decoder v·ªõi Residual Blocks**:

```python
# Encoder (downsampling)
- ReflectionPad2d(3) + Conv2d(1‚Üí64, kernel=7) + InstanceNorm2d + ReLU
- Conv2d(64‚Üí128, kernel=3, stride=2) + InstanceNorm2d + ReLU  # 1/2 resolution
- Conv2d(128‚Üí256, kernel=3, stride=2) + InstanceNorm2d + ReLU # 1/4 resolution

# Residual Blocks (9 blocks)
- 9 √ó ResidualBlock(256 features)

# Decoder (upsampling)  
- ConvTranspose2d(256‚Üí128, kernel=3, stride=2) + InstanceNorm2d + ReLU # 1/2 resolution
- ConvTranspose2d(128‚Üí64, kernel=3, stride=2) + InstanceNorm2d + ReLU  # full resolution
- ReflectionPad2d(3) + Conv2d(64‚Üí1, kernel=7) + Tanh
```

**L√Ω do thi·∫øt k·∫ø**:
- **9 Residual Blocks**: ƒê·ªß s√¢u ƒë·ªÉ h·ªçc complex mappings, tr√°nh vanishing gradient
- **ReflectionPad**: Gi·∫£m artifacts ·ªü bi√™n ·∫£nh so v·ªõi zero padding
- **InstanceNorm**: Ph√π h·ª£p v·ªõi style transfer, t·ªët h∆°n BatchNorm cho medical images
- **Tanh activation**: Output trong [-1, 1], ph√π h·ª£p v·ªõi normalized input

### PatchGAN Discriminator

**Thi·∫øt k·∫ø 70√ó70 PatchGAN**:
```python
# Layer 1: Conv2d(1‚Üí64, kernel=4, stride=2) + LeakyReLU(0.2)     # No normalization
# Layer 2: Conv2d(64‚Üí128, kernel=4, stride=2) + InstanceNorm + LeakyReLU(0.2)
# Layer 3: Conv2d(128‚Üí256, kernel=4, stride=2) + InstanceNorm + LeakyReLU(0.2)  
# Layer 4: Conv2d(256‚Üí512, kernel=4, stride=1) + InstanceNorm + LeakyReLU(0.2)
# Output:  Conv2d(512‚Üí1, kernel=4, stride=1)                     # No activation
```

**∆Øu ƒëi·ªÉm PatchGAN**:
- **Local discrimination**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ·ªü m·ª©c local patches (70√ó70)
- **Fewer parameters**: Hi·ªáu qu·∫£ h∆°n full-image discriminator
- **Better texture quality**: T·∫≠p trung v√†o chi ti·∫øt texture thay v√¨ global structure

## üìä Chi ti·∫øt Loss Functions v√† Tr·ªçng s·ªë

### 1. Adversarial Loss (Œª = 1.0)

**C√¥ng th·ª©c**:
```python
loss_gan_ct = F.mse_loss(D_CT(fake_ct), torch.ones_like(D_CT(fake_ct)))
loss_gan_mri = F.mse_loss(D_MRI(fake_mri), torch.ones_like(D_MRI(fake_mri)))
loss_gan = (loss_gan_ct + loss_gan_mri) * 0.5
```

**L√Ω do ch·ªçn MSE thay v√¨ BCE**:
- **Least-squares GAN**: ·ªîn ƒë·ªãnh training h∆°n, √≠t b·ªã mode collapse
- **Smoother gradients**: Gradient penalty t·ª± nhi√™n g·∫ßn decision boundary
- **Better convergence**: Ph√π h·ª£p v·ªõi medical image domain

**Tr·ªçng s·ªë Œª_adversarial = 1.0**:
- **Baseline weight**: L√†m reference cho c√°c loss kh√°c
- **Balanced contribution**: Kh√¥ng qu√° dominant so v·ªõi cycle consistency

### 2. Cycle Consistency Loss (Œª = 10.0)

**C√¥ng th·ª©c**:
```python
loss_cycle_mri = F.l1_loss(G_CT2MRI(G_MRI2CT(mri)), mri)
loss_cycle_ct = F.l1_loss(G_MRI2CT(G_CT2MRI(ct)), ct)
loss_cycle = (loss_cycle_mri + loss_cycle_ct) * 0.5
```

**L√Ω do tr·ªçng s·ªë cao (Œª = 10.0)**:
- **Core constraint**: ƒê·∫£m b·∫£o t√≠nh consistency c∆° b·∫£n c·ªßa cycle
- **Prevent mode collapse**: NgƒÉn generator t·∫°o ra mapping t√πy √Ω
- **Medical accuracy**: ƒê·∫∑c bi·ªát quan tr·ªçng trong ·ª©ng d·ª•ng y t·∫ø
- **Empirical validation**: Gi√° tr·ªã 10.0 ƒë∆∞·ª£c verify qua nhi·ªÅu nghi√™n c·ª©u CycleGAN

**L1 vs L2 loss**:
- **L1 Loss**: √çt b·ªã blur h∆°n, preserve sharp edges t·ªët h∆°n
- **Robust to outliers**: Quan tr·ªçng v·ªõi medical images c√≥ noise

### 3. Identity Loss (Œª = 5.0)

**C√¥ng th·ª©c**:
```python
loss_identity_ct = F.l1_loss(G_MRI2CT(ct), ct)
loss_identity_mri = F.l1_loss(G_CT2MRI(mri), mri)
loss_identity = (loss_identity_ct + loss_identity_mri) * 0.5
```

**M·ª•c ƒë√≠ch**:
- **Color preservation**: Gi·ªØ nguy√™n t√¥ng m√†u khi input ƒë√£ ƒë√∫ng domain
- **Reduce overshoot**: Tr√°nh generator thay ƒë·ªïi qu√° m·ª©c kh√¥ng c·∫ßn thi·∫øt

**Tr·ªçng s·ªë Œª_identity = 5.0**:
- **Moderate constraint**: Kh√¥ng qu√° restrictive nh∆∞ cycle consistency
- **Medical consideration**: ƒê·∫£m b·∫£o intensity values ƒë∆∞·ª£c preserve ph√π h·ª£p
- **Half of cycle weight**: C√¢n b·∫±ng gi·ªØa consistency v√† flexibility

### 4. Perceptual Loss (Œª = 1.0)

**Ki·∫øn tr√∫c VGG19-based**:
```python
feature_layers = [3, 8, 15, 22]  # ReLU1_2, ReLU2_2, ReLU3_4, ReLU4_4
loss_perceptual = Œ£ MSE(VGG_i(fake_ct), VGG_i(real_ct))
```

**L√Ω do ch·ªçn VGG19**:
- **Pre-trained features**: ƒê√£ h·ªçc ƒë∆∞·ª£c generic visual patterns
- **Multi-scale representation**: T·ª´ low-level ƒë·∫øn high-level features
- **Medical imaging validation**: Hi·ªáu qu·∫£ ƒë√£ ƒë∆∞·ª£c ch·ª©ng minh v·ªõi medical images

**Feature layers selection**:
- **Layer 3 (ReLU1_2)**: Low-level textures, edges
- **Layer 8 (ReLU2_2)**: Medium-level patterns  
- **Layer 15 (ReLU3_4)**: High-level features
- **Layer 22 (ReLU4_4)**: Semantic representations

**Tr·ªçng s·ªë Œª_perceptual = 1.0**:
- **Complementary role**: B·ªï sung cho pixel-wise losses
- **Equal importance**: V·ªõi adversarial loss ƒë·ªÉ c√¢n b·∫±ng quality
- **Medical relevance**: ƒê·∫£m b·∫£o similarity ·ªü multiple perception levels

### 5. T·ªïng h·ª£p Loss Function

**Total Generator Loss**:
```python
L_total = Œª_adversarial √ó L_GAN + 
          Œª_cycle √ó L_cycle + 
          Œª_identity √ó L_identity + 
          Œª_perceptual √ó L_perceptual

L_total = 1.0 √ó L_GAN + 10.0 √ó L_cycle + 5.0 √ó L_identity + 1.0 √ó L_perceptual
```

**Hierarchy c·ªßa tr·ªçng s·ªë**:
1. **L_cycle (10.0)**: Highest priority - ƒë·∫£m b·∫£o consistency c∆° b·∫£n
2. **L_identity (5.0)**: Medium priority - preserve original characteristics  
3. **L_GAN (1.0)**: Standard priority - realistic generation
4. **L_perceptual (1.0)**: Standard priority - perceptual quality

**T√°c ƒë·ªông c·ªßa t·ª´ng th√†nh ph·∫ßn**:
- **Cycle consistency**: ƒê·∫£m b·∫£o mapping c√≥ th·ªÉ ƒë·∫£o ng∆∞·ª£c
- **Identity loss**: Gi·ªØ nguy√™n khi kh√¥ng c·∫ßn transform
- **Adversarial loss**: T·∫°o ra ·∫£nh realistic
- **Perceptual loss**: C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng visual v√† texture

## üìà Metrics ƒê√°nh gi√°

### 1. Mean Absolute Error (MAE)
- **C√¥ng th·ª©c**: `MAE = (1/N) Œ£ |y_pred - y_true|`
- **√ù nghƒ©a**: Sai s·ªë trung b√¨nh tuy·ªát ƒë·ªëi, ƒë∆°n v·ªã gi·ªëng pixel intensity
- **T·∫ßm quan tr·ªçng**: ƒêo l∆∞·ªùng ƒë·ªô ch√≠nh x√°c pixel-level

### 2. Mean Squared Error (MSE)  
- **C√¥ng th·ª©c**: `MSE = (1/N) Œ£ (y_pred - y_true)¬≤`
- **√ù nghƒ©a**: Nh·∫•n m·∫°nh c√°c sai s·ªë l·ªõn, penalty cho outliers
- **S·ª≠ d·ª•ng**: ƒê√°nh gi√° overall reconstruction quality

### 3. Peak Signal-to-Noise Ratio (PSNR)
- **C√¥ng th·ª©c**: `PSNR = 20 √ó log‚ÇÅ‚ÇÄ(MAX_I / ‚àöMSE)`
- **ƒê∆°n v·ªã**: Decibel (dB)
- **Gi√° tr·ªã t·ªët**: >25 dB cho medical images
- **√ù nghƒ©a**: T·ª∑ l·ªá signal/noise, cao h∆°n = ch·∫•t l∆∞·ª£ng t·ªët h∆°n

### 4. Structural Similarity Index (SSIM)
- **Range**: [0, 1], c√†ng g·∫ßn 1 c√†ng t·ªët
- **Th√†nh ph·∫ßn**: Luminance √ó Contrast √ó Structure
- **∆Øu ƒëi·ªÉm**: T∆∞∆°ng quan t·ªët v·ªõi perception c·ªßa con ng∆∞·ªùi
- **Medical relevance**: Quan tr·ªçng cho texture v√† structural details

### 5. Normalized Cross Correlation (NCC)
- **Range**: [-1, 1], l√Ω t∆∞·ªüng l√† 1
- **√ù nghƒ©a**: ƒêo correlation gi·ªØa hai ·∫£nh
- **Robust**: Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi linear intensity changes

## üöÄ H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

### C√†i ƒë·∫∑t Dependencies

```bash
git clone <repository-url>
cd mri-to-ct
pip install -r requirements.txt
```

### Training

```bash
cd src
python train.py
```

**Tham s·ªë Training c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh**:
```python
config = {
    'batch_size': 4,          # T√πy thu·ªôc GPU memory
    'num_epochs': 200,        # ƒê·ªß ƒë·ªÉ converge
    'lr_G': 0.0002,          # Learning rate cho Generator
    'lr_D': 0.0002,          # Learning rate cho Discriminator  
    'decay_epoch': 100,       # B·∫Øt ƒë·∫ßu decay LR
    'decay_epochs': 100       # S·ªë epoch ƒë·ªÉ decay v·ªÅ 0
}
```

### Testing

#### Test m·ªôt ·∫£nh MRI ƒë∆°n l·∫ª:
```bash
python test.py --model_path checkpoints/best_model.pth \
               --test_mode single \
               --mri_path data/MRI/brain_001.nii.gz \
               --output_dir results/
```

#### Test v·ªõi ground truth ƒë·ªÉ t√≠nh metrics:
```bash
python test.py --model_path checkpoints/best_model.pth \
               --test_mode with_gt \
               --mri_path data/MRI/brain_001.nii.gz \
               --ct_path data/CT/brain_001.nii.gz \
               --output_dir results/
```

#### Test tr√™n to√†n b·ªô dataset:
```bash
python test.py --model_path checkpoints/best_model.pth \
               --test_mode dataset \
               --mri_dir data/Test/MRI \
               --ct_dir data/Test/CT \
               --output_dir results/
```

## üìÅ D·ªØ li·ªáu Input/Output

### D·ªØ li·ªáu ƒë·∫ßu v√†o
- **Format**: NIfTI (.nii.gz)
- **Naming convention**: brain_001.nii.gz ƒë·∫øn brain_046.nii.gz
- **Pairing**: M·ªói file MRI c√≥ file CT t∆∞∆°ng ·ª©ng c√πng t√™n
- **Preprocessing**: D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c c·∫Øt v√πng (cropped) th·ªß c√¥ng
- **Resolution**: T·ª± ƒë·ªông resize v·ªÅ 256√ó256 cho training

### K·∫øt qu·∫£ ƒë·∫ßu ra
- **Synthetic CT**: File .nii.gz v·ªõi suffix "_synthetic_ct"
- **Comparison images**: File .png hi·ªÉn th·ªã MRI input, CT m√¥ ph·ªèng, v√† difference map
- **Metrics report**: C√°c ch·ªâ s·ªë MAE, MSE, SSIM, PSNR, NCC
- **Tensorboard logs**: Theo d√µi loss v√† metrics qua epochs

## ‚öôÔ∏è Y√™u c·∫ßu H·ªá th·ªëng

### Hardware Requirements
- **GPU**: CUDA-enabled GPU v·ªõi √≠t nh·∫•t 8GB VRAM (khuy·∫øn ngh·ªã 16GB+)
- **RAM**: T·ªëi thi·ªÉu 16GB, khuy·∫øn ngh·ªã 32GB
- **Storage**: ~20GB cho d·ªØ li·ªáu, checkpoints v√† logs
- **CPU**: Multi-core processor ƒë·ªÉ data loading

### Software Requirements
- **Python**: 3.7+
- **PyTorch**: 1.9.0+
- **CUDA**: 11.0+ (n·∫øu s·ª≠ d·ª•ng GPU)
- **Operating System**: Linux (khuy·∫øn ngh·ªã), Windows, macOS

## üìä Monitoring v√† Debugging

### Tensorboard Monitoring
```bash
tensorboard --logdir logs/
```

**Metrics ƒë∆∞·ª£c track**:
- Training/Validation losses (G_total, G_gan, G_cycle, G_identity, G_perceptual)
- Discriminator losses (D_CT, D_MRI)
- Image quality metrics (MAE, MSE, SSIM, PSNR)
- Learning rates

### Sample Images
- **Frequency**: M·ªói 1 epochs
- **Location**: `samples/epoch_X/`
- **Content**: Real MRI, Fake CT, Real CT, Reconstructed images

### Checkpoint Strategy
- **Regular saves**: M·ªói 10 epochs
- **Best model**: D·ª±a tr√™n validation SSIM
- **Resume capability**: C√≥ th·ªÉ ti·∫øp t·ª•c training t·ª´ checkpoint

## üî¨ K·∫øt qu·∫£ Mong ƒë·ª£i

### Performance Benchmarks
- **SSIM**: >0.85 cho high-quality synthesis
- **PSNR**: >25 dB
- **MAE**: <0.1 (v·ªõi normalized intensity)
- **Training time**: 24-48 gi·ªù tr√™n GPU RTX 3080

### Qualitative Assessment
- **Bone structures**: R√µ n√©t v√† ch√≠nh x√°c trong CT m√¥ ph·ªèng
- **Soft tissue contrast**: T∆∞∆°ng ƒë·ªìng v·ªõi CT th·∫≠t
- **Artifacts**: T·ªëi thi·ªÉu noise v√† distortion
- **Anatomical consistency**: Gi·ªØ nguy√™n c·∫•u tr√∫c t·ª´ MRI


## üìö T√†i li·ªáu Tham kh·∫£o

### Core Papers
- **CycleGAN**: Zhu, J.Y., et al. "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks." ICCV 2017
- **Perceptual Loss**: Johnson, J., et al. "Perceptual Losses for Real-Time Style Transfer and Super-Resolution." ECCV 2016
- **PatchGAN**: Isola, P., et al. "Image-to-Image Translation with Conditional Adversarial Networks." CVPR 2017

### Medical Imaging References
- **MRI-CT Synthesis**: Lei, Y., et al. "MRI-only based synthetic CT generation using dense cycle consistent generative adversarial networks." Medical Physics 2019
- **N4 Bias Correction**: Tustison, N.J., et al. "N4ITK: improved N3 bias correction." IEEE TMI 2010

### Implementation Guides
- **README Best Practices**: [How to Write a Good README](https://www.freecodecamp.org/news/how-to-write-a-good-readme-file/)
- **Documentation Standards**: [Make a README](https://www.makeareadme.com/)


## 2. Data Preprocessing Pipeline

### 2.1 MRI-Guided Artifact Removal (NEW APPROACH)

**Rationale**: ·∫¢nh CT th∆∞·ªùng ch·ª©a c√°c artifacts t·ª´ couch v√† headframe trong qu√° tr√¨nh ch·ª•p, trong khi ·∫£nh MRI th√¨ s·∫°ch h∆°n v√¨ kh√¥ng c√≥ c√°c thi·∫øt b·ªã n√†y. V√¨ MRI v√† CT ƒë√£ ƒë∆∞·ª£c fusion v·ªõi nhau, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng brain mask t·ª´ MRI ƒë·ªÉ lo·∫°i b·ªè artifacts t·ª´ CT.

**Pipeline Steps**:

1. **N4 Bias Field Correction (ch·ªâ cho MRI)**:
   ```python
   corrector = sitk.N4BiasFieldCorrectionImageFilter()
   corrector.SetMaximumNumberOfIterations([50] * 4)
   ```
   - Lo·∫°i b·ªè bias field trong MRI ƒë·ªÉ c√≥ brain mask ch√≠nh x√°c h∆°n

2. **MRI Brain Mask Creation**:
   ```python
   def _create_mri_brain_mask(self, mri_array):
       # Multi-step thresholding
       otsu_thresh = filters.threshold_otsu(normalized)
       brain_thresh = otsu_thresh * 0.7  # Capture gray matter
       
       # Morphological cleanup
       initial_mask = morphology.remove_small_objects(mask, min_size=2000)
       initial_mask = ndimage.binary_fill_holes(initial_mask)
       
       # Largest connected component (main brain)
       labeled_mask = measure.label(initial_mask)
       largest_component = np.argmax(np.bincount(labeled_mask.ravel())[1:]) + 1
       refined_mask = (labeled_mask == largest_component)
   ```
   - S·ª≠ d·ª•ng Otsu thresholding v·ªõi threshold th·∫•p h∆°n (0.7x) ƒë·ªÉ capture gray matter
   - Lo·∫°i b·ªè small objects v√† fill holes
   - L·∫•y largest connected component ƒë·ªÉ c√≥ brain mask ch√≠nh x√°c

3. **Apply MRI Mask to CT**:
   ```python
   def _apply_mri_mask_to_ct(self, ct_array, mri_mask):
       # Set background (couch/headframe) to appropriate value
       background_value = np.percentile(ct_array[mri_mask == 0], 25)
       masked_ct[mri_mask == 0] = background_value
       
       # Additional artifact removal in brain region
       # Target extreme metal artifacts conservatively
       q99 = np.percentile(brain_region, 99)
       metal_mask = (masked_ct > q99 * 1.5) & (mri_mask > 0)
       # Replace with median brain tissue value
   ```
   - Lo·∫°i b·ªè couch/headframe b·∫±ng c√°ch set v√πng ngo√†i brain mask th√†nh background value
   - X·ª≠ l√Ω th√™m metal artifacts trong brain region m·ªôt c√°ch conservative

4. **Brain Contrast Enhancement**:
   ```python
   def _enhance_brain_contrast(self, image_array, mask, modality):
       if modality == 'CT':
           # Percentile stretching + gentle gamma correction
           p2, p98 = np.percentile(brain_values, [2, 98])
           gamma = 0.9  # Slightly enhance contrast
           brain_corrected = np.power(brain_normalized, gamma)
   ```
   - Enhance soft tissue contrast trong CT
   - Preserve MRI characteristics

5. **Robust Normalization**:
   ```python
   def _normalize_intensity(self, image_array, mask, modality):
       if modality == 'CT':
           min_val = np.percentile(masked_values, 1)
           max_val = np.percentile(masked_values, 99)
       else:  # MRI
           min_val = np.percentile(masked_values, 2)
           max_val = np.percentile(masked_values, 98)
   ```
   - S·ª≠ d·ª•ng percentile-based normalization ƒë·ªÉ robust v·ªõi outliers
   - CT: wider percentile range ƒë·ªÉ preserve tissue contrast
   - MRI: standard range

**Advantages c·ªßa MRI-Guided Approach**:
- ‚úÖ **Effective artifact removal**: Lo·∫°i b·ªè couch/headframe m·ªôt c√°ch ch√≠nh x√°c
- ‚úÖ **Preserve brain tissue**: Kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn brain tissue contrast
- ‚úÖ **Consistent masking**: S·ª≠ d·ª•ng c√πng mask cho c·∫£ MRI v√† CT ƒë·∫£m b·∫£o consistency
- ‚úÖ **Robust to registration errors**: C√≥ th·ªÉ handle m·ªôt √≠t sai l·ªách registration
- ‚úÖ **Conservative approach**: Ch·ªâ target extreme artifacts, preserve normal tissue

**Key Parameters** (Updated for improved version):
- `brain_thresh = otsu_thresh * 0.7`: Capture gray matter  
- `min_size=2000`: Remove small noise objects
- `metal_threshold = q95 + 2*(q95-q50)`: Robust metal artifact detection
- `air_threshold = q05 - 2*(q50-q05)`: Robust air artifact detection
- `outlier_removal`: 3*IQR for CT, 2.5*IQR for MRI (NO gamma correction)
- `normalization`: Tissue-aware v·ªõi HU preservation cho CT

### 2.1.1 Performance Improvements (Final Version)

**Quantitative Results t·ª´ testing**:
- ‚úÖ **Brain mask coverage**: 40.9% (optimal cho brain tissue)
- ‚úÖ **Artifact removal**: 12.7% pixels changed by masking (couch/headframe)  
- ‚úÖ **Outlier clipping**: 3.9% pixels changed (gentle artifact removal)
- ‚úÖ **Mask consistency**: IoU = 1.000 (perfect between MRI-CT)
- ‚úÖ **Contrast preservation**: 210.7% (enhanced natural contrast)
- ‚úÖ **Final range**: [0, 1] normalized, then [-1, 1] for training

**Key Improvements over Previous Methods**:

1. **Better Artifact Detection**:
   ```python
   # OLD: Simple percentile thresholds
   metal_threshold = q99 * 1.5
   
   # NEW: Robust statistical approach  
   metal_threshold = q95 + 2 * (q95 - q50)
   air_threshold = q05 - 2 * (q50 - q05)
   ```

2. **Natural Contrast Preservation**:
   ```python
   # OLD: Gamma correction + histogram equalization
   gamma = 0.9
   brain_corrected = np.power(brain_normalized, gamma)
   
   # NEW: Minimal outlier clipping only
   # 3 * IQR rule cho CT, 2.5 * IQR cho MRI
   # NO gamma correction - preserve HU relationships
   ```

3. **Tissue-Aware Normalization**:
   ```python
   # OLD: Simple percentile normalization
   min_val = np.percentile(masked_values, 1)
   max_val = np.percentile(masked_values, 99)
   
   # NEW: CT tissue-specific approach
   # Ensure minimum dynamic range (50 HU)
   # Use mean ¬± 3*std if range too narrow
   # Preserve brain tissue HU relationships
   ```

**Clinical Relevance**: 
- CT HU values preserved for tissue differentiation
- MRI tissue contrast maintained naturally  
- Artifacts removed without destroying normal anatomy
- Suitable for medical imaging applications requiring preservation of tissue characteristics

### 2.2 Previous Approaches (For Reference)

#### 2.2.1 Otsu Thresholding
- **Purpose**: T√°ch brain tissue t·ª´ background
- **Method**: Automatic threshold selection d·ª±a tr√™n histogram
- **Parameters**: `min_size=1000` ƒë·ªÉ lo·∫°i b·ªè small objects

#### 2.2.2 N4ITK Bias Correction
- **Purpose**: Lo·∫°i b·ªè intensity non-uniformity trong MRI
- **Parameters**: 50 iterations √ó 4 levels = 200 total iterations
- **Rationale**: Medical imaging standard, ƒë·∫£m b·∫£o uniform intensity