#!/usr/bin/env python3
"""
Script ki·ªÉm tra GPU GTX 1650 v√† memory cho training MRI-to-CT
"""

import torch
import torch.nn as nn

def check_gpu_status():
    """Ki·ªÉm tra tr·∫°ng th√°i GPU chi ti·∫øt"""
    print("=" * 50)
    print("KI·ªÇM TRA GPU V√Ä CUDA")
    print("=" * 50)
    
    # Basic CUDA info
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA kh√¥ng kh·∫£ d·ª•ng!")
        return False
    
    print(f"CUDA version: {torch.version.cuda}")
    print(f"Device count: {torch.cuda.device_count()}")
    print(f"Current device: {torch.cuda.current_device()}")
    
    # GPU details
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"  Total memory: {props.total_memory / 1024**3:.1f} GB")
        print(f"  Major: {props.major}, Minor: {props.minor}")
        print(f"  Multi-processor count: {props.multi_processor_count}")
    
    # Memory status
    print(f"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB")
    print(f"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB")
    
    return True

def test_model_memory():
    """Test memory usage v·ªõi model t∆∞∆°ng t·ª±"""
    print("\n" + "=" * 50)
    print("TEST MEMORY USAGE V·ªöI MODEL MRI-TO-CT")
    print("=" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        # Test v·ªõi model nh·ªè tr∆∞·ªõc
        print("Testing v·ªõi model Generator nh·ªè...")
        
        # T·∫°o model test nh·ªè
        model = nn.Sequential(
            nn.Conv2d(1, 64, 7, padding=3),
            nn.InstanceNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.InstanceNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, stride=2, padding=1),
            nn.InstanceNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 1, 7, padding=3),
            nn.Tanh()
        ).to(device)
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Test v·ªõi batch size kh√°c nhau
        batch_sizes = [1, 2, 4]
        input_size = (256, 256)
        
        for batch_size in batch_sizes:
            try:
                torch.cuda.empty_cache()
                
                # T·∫°o input tensor
                x = torch.randn(batch_size, 1, *input_size).to(device)
                
                # Forward pass
                with torch.no_grad():
                    y = model(x)
                
                memory_used = torch.cuda.memory_allocated(0) / 1024**3
                print(f"‚úÖ Batch size {batch_size}: {memory_used:.2f} GB")
                
                # Th·ª≠ backward pass
                y = model(x)
                loss = y.mean()
                loss.backward()
                
                memory_used = torch.cuda.memory_allocated(0) / 1024**3
                print(f"‚úÖ Batch size {batch_size} (with backward): {memory_used:.2f} GB")
                
                del x, y, loss
                torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"‚ùå Batch size {batch_size}: Out of memory")
                else:
                    print(f"‚ùå Batch size {batch_size}: {e}")
                    
        print("\nüìä RECOMMENDATIONS:")
        print("- Batch size 1: An to√†n cho GTX 1650 4GB")
        print("- Batch size 2: C√≥ th·ªÉ ho·∫°t ƒë·ªông v·ªõi model nh·ªè")
        print("- Batch size 4+: C√≥ th·ªÉ out of memory")
        
        return True
        
    except Exception as e:
        print(f"‚ùå L·ªói test memory: {e}")
        return False

def estimate_cyclegan_memory():
    """∆Ø·ªõc t√≠nh memory cho CycleGAN"""
    print("\n" + "=" * 50)
    print("∆Ø·ªöC T√çNH MEMORY CHO CYCLEGAN")
    print("=" * 50)
    
    # Th√¥ng s·ªë model
    configs = [
        {"name": "Small (6 res blocks)", "res_blocks": 6, "batch_size": 1},
        {"name": "Medium (9 res blocks)", "res_blocks": 9, "batch_size": 1},
        {"name": "Large (12 res blocks)", "res_blocks": 12, "batch_size": 1},
    ]
    
    for config in configs:
        # ∆Ø·ªõc t√≠nh parameters
        # Generator: ~11M params per 3 residual blocks
        gen_params = config["res_blocks"] * 3.7e6  
        total_params = gen_params * 2 + 2.7e6  # 2 generators + 2 discriminators
        
        # ∆Ø·ªõc t√≠nh memory (rough estimate)
        # Model weights: 4 bytes per param
        # Forward activations: ~batch_size * 256 * 256 * channels * 4 bytes * multiple layers
        # Backward gradients: 2x forward
        
        model_memory = total_params * 4 / 1024**3  # GB
        activation_memory = config["batch_size"] * 256 * 256 * 64 * 4 * 10 / 1024**3  # GB
        total_estimated = model_memory + activation_memory * 3  # 3x for forward+backward+optimizer
        
        print(f"{config['name']}:")
        print(f"  Parameters: {total_params/1e6:.1f}M")
        print(f"  Estimated memory: {total_estimated:.2f} GB")
        
        if total_estimated < 3.5:
            print(f"  Status: ‚úÖ An to√†n cho GTX 1650")
        elif total_estimated < 4.0:
            print(f"  Status: ‚ö†Ô∏è  C√≥ th·ªÉ ho·∫°t ƒë·ªông")
        else:
            print(f"  Status: ‚ùå C√≥ th·ªÉ out of memory")
        print()

def main():
    """Main function"""
    print("KI·ªÇM TRA GPU CHO TRAINING MRI-TO-CT CYCLEGAN")
    print("GPU: NVIDIA GTX 1650 (4GB VRAM)")
    
    if not check_gpu_status():
        return
    
    test_model_memory()
    estimate_cyclegan_memory()
    
    print("\n" + "=" * 50)
    print("KHUY·∫æN NGH·ªä FINAL:")
    print("=" * 50)
    print("‚úÖ S·ª≠ d·ª•ng batch_size = 1")
    print("‚úÖ S·ª≠ d·ª•ng 6 residual blocks thay v√¨ 9")
    print("‚úÖ Monitor GPU memory trong qu√° tr√¨nh training")
    print("‚úÖ S·ª≠ d·ª•ng torch.cuda.empty_cache() ƒë·ªãnh k·ª≥")
    print("‚ö†Ô∏è  N·∫øu out of memory, gi·∫£m image size xu·ªëng 128x128")

if __name__ == "__main__":
    main() 