# ğŸš€ Fast MRI-to-CT Training System

## Tá»•ng quan

Há»‡ thá»‘ng training nhanh nÃ y tÃ¡ch **preprocessing** ra khá»i **training loop** Ä‘á»ƒ tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ quÃ¡ trÃ¬nh training. Thay vÃ¬ preprocessing láº¡i 42 bá»‡nh nhÃ¢n má»—i epoch, chÃºng ta:

1. **Preprocessing má»™t láº§n** vÃ  lÆ°u cache 
2. **Training nhanh** chá»‰ vá»›i augmentation

**Káº¿t quáº£**: Training nhanh hÆ¡n **~450x** so vá»›i preprocessing realtime!

## ğŸ“Š Táº¡i sao nhanh hÆ¡n?

### TrÆ°á»›c Ä‘Ã¢y (cháº­m):
```
Má»—i epoch: Preprocessing (42 Ã— 45s) + Training = ~30 phÃºt/epoch
100 epochs = ~50 giá»
```

### BÃ¢y giá» (nhanh):
```
Preprocessing 1 láº§n: ~30 phÃºt
Má»—i epoch: Load cache + Augmentation = ~4s/epoch  
100 epochs = 30 phÃºt + 7 phÃºt = ~37 phÃºt tá»•ng cá»™ng
```

## ğŸ› ï¸ CÃ¡ch sá»­ dá»¥ng

### 1. Cháº¡y toÃ n bá»™ pipeline (Ä‘Æ¡n giáº£n nháº¥t):

```bash
# Cháº¡y cáº£ preprocessing + training
python run_fast_training.py --epochs 50 --batch_size 6 --use_amp

# Vá»›i custom directories
python run_fast_training.py \
    --mri_dir data/MRI \
    --ct_dir data/CT \
    --cache_dir preprocessed_cache \
    --output_dir fast_training_output \
    --epochs 100 \
    --batch_size 8 \
    --use_amp
```

### 2. Hoáº·c cháº¡y tá»«ng bÆ°á»›c:

#### BÆ°á»›c 1: Preprocessing vÃ  cache (má»™t láº§n)
```bash
# Preprocess vÃ  lÆ°u cache HDF5
python preprocess_and_cache.py --format h5

# Hoáº·c lÆ°u cache PyTorch tensors (nhanh hÆ¡n loading)
python preprocess_and_cache.py --format pt
```

#### BÆ°á»›c 2: Training nhanh vá»›i cache
```bash
# Training vá»›i cached data
python train_with_cache.py \
    --cache_dir preprocessed_cache \
    --batch_size 8 \
    --epochs 100 \
    --use_amp
```

### 3. Test tá»‘c Ä‘á»™ cache:
```bash
python -c "from src.cached_data_loader import test_cached_loader_speed; test_cached_loader_speed()"
```

## ğŸ“ Cáº¥u trÃºc cache

```
preprocessed_cache/
â”œâ”€â”€ brain_001.h5              # Preprocessed volumes
â”œâ”€â”€ brain_002.h5
â”œâ”€â”€ ...
â””â”€â”€ metadata/
    â””â”€â”€ preprocessing_info.pkl # Metadata vÃ  thá»‘ng kÃª
```

## âš™ï¸ Tham sá»‘ training Ä‘Æ°á»£c tá»‘i Æ°u

### Google Colab (Tesla T4):
```bash
python run_fast_training.py \
    --batch_size 4 \
    --epochs 100 \
    --workers 2 \
    --use_amp \
    --save_freq 5
```

### GPU máº¡nh hÆ¡n:
```bash
python run_fast_training.py \
    --batch_size 8 \
    --epochs 100 \
    --workers 4 \
    --use_amp \
    --save_freq 5
```

## ğŸ”§ Options chi tiáº¿t

### Preprocessing options:
- `--cache_format`: `h5` (HDF5, Ã­t dung lÆ°á»£ng) hoáº·c `pt` (PyTorch, nhanh hÆ¡n)
- `--skip_preprocessing`: Bá» qua preprocessing náº¿u cache Ä‘Ã£ tá»“n táº¡i

### Training options:
- `--batch_size`: Batch size (cÃ³ thá»ƒ lá»›n hÆ¡n vá»›i cache)
- `--epochs`: Sá»‘ epochs 
- `--lr`: Learning rate (default: 0.0002)
- `--workers`: Sá»‘ DataLoader workers
- `--use_amp`: Sá»­ dá»¥ng mixed precision (tiáº¿t kiá»‡m memory)
- `--aug_prob`: XÃ¡c suáº¥t augmentation (default: 0.8)

### Save options:
- `--save_freq`: Táº§n suáº¥t lÆ°u checkpoint (epochs)
- `--val_freq`: Táº§n suáº¥t validation (epochs)

## ğŸ“ˆ Monitoring training

Training sáº½ hiá»ƒn thá»‹:
```
Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 347/347 [00:04<00:00, 89.2it/s]
G_loss: 0.8234, D_loss: 0.1567, Total: 0.9801

ğŸ“Š Epoch 1/100 Summary:
   Train Loss: 0.9801
   Val Loss: 0.8543
   Epoch Time: 4.2s
   Est. Remaining: 6.8 minutes
```

## ğŸ¯ Káº¿t quáº£

Sau training, báº¡n sáº½ cÃ³:

```
fast_training_output/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.pth        # Model tá»‘t nháº¥t
â”‚   â”œâ”€â”€ epoch_5.pth          # Checkpoints Ä‘á»‹nh ká»³
â”‚   â””â”€â”€ epoch_10.pth
â”œâ”€â”€ samples/                  # Sample outputs
â””â”€â”€ logs/                    # Training logs
```

## ğŸ” Kiá»ƒm tra mÃ´i trÆ°á»ng

```bash
# Kiá»ƒm tra mÃ´i trÆ°á»ng trÆ°á»›c khi cháº¡y
python run_fast_training.py --check_env
```

Output:
```
ğŸ” CHECKING ENVIRONMENT
âœ… GPU: NVIDIA GeForce RTX 3080 (10.0GB)
âœ… RAM: 32.0GB (available: 28.5GB)
âœ… Disk: 500.2GB free
âœ… numpy
âœ… torch
âœ… h5py
âœ… Environment check passed!
```

## ğŸ’¡ Tips tá»‘i Æ°u

### 1. Chá»n cache format:
- **HDF5 (`h5`)**: Ãt dung lÆ°á»£ng, load hÆ¡i cháº­m hÆ¡n
- **PyTorch (`pt`)**: Nhiá»u dung lÆ°á»£ng hÆ¡n, load nhanh nháº¥t

### 2. Batch size:
- **Tesla T4 (15GB)**: batch_size=4-6
- **RTX 3080 (10GB)**: batch_size=6-8  
- **RTX 4090 (24GB)**: batch_size=12-16

### 3. Workers:
- **Local machine**: 4-8 workers
- **Google Colab**: 2 workers (optimal)

### 4. Mixed precision:
- LuÃ´n dÃ¹ng `--use_amp` Ä‘á»ƒ tiáº¿t kiá»‡m memory vÃ  tÄƒng tá»‘c

## âš ï¸ LÆ°u Ã½

1. **Cache size**: ~2-5GB cho 42 bá»‡nh nhÃ¢n
2. **Preprocessing**: Máº¥t ~30-60 phÃºt má»™t láº§n duy nháº¥t
3. **Training**: ~4-8s/epoch vá»›i cache vs ~30 phÃºt/epoch khÃ´ng cache
4. **Augmentation**: Váº«n random má»—i epoch cho diversity

## ğŸ†˜ Troubleshooting

### Lá»—i memory:
```bash
# Giáº£m batch size
--batch_size 2

# Giáº£m workers  
--workers 1

# DÃ¹ng mixed precision
--use_amp
```

### Cache bá»‹ há»ng:
```bash
# XÃ³a cache vÃ  táº¡o láº¡i
rm -rf preprocessed_cache
python preprocess_and_cache.py
```

### Training cháº­m:
```bash
# Kiá»ƒm tra cache format
--cache_format pt  # Nhanh hÆ¡n h5

# TÄƒng workers náº¿u cÃ³ Ä‘á»§ CPU
--workers 4
```

## ğŸ“š Technical Details

### Cache preprocessing steps:
1. N4 bias correction (MRI)
2. Brain+skull mask creation  
3. MRI-guided CT artifact removal
4. Outlier clipping
5. Intensity normalization
6. Brain ROI cropping
7. Resize to 256Ã—256
8. Range clamping [0,1]

### Fast augmentation (training time):
1. Convert to [-1,1] range (for model input)
2. Random rotation (Â±15Â°) with background=-1
3. Random horizontal flip
4. Random vertical flip  
5. Random intensity scaling (Â±10%) in [-1,1] range

### Memory optimization:
- HDF5 compression (gzip level 9)
- Mixed precision training
- Gradient checkpointing
- Smart memory cleanup

---

ğŸš€ **Káº¿t quáº£**: Training 100 epochs tá»« ~50 giá» xuá»‘ng ~40 phÃºt! 