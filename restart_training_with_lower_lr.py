#!/usr/bin/env python3
"""
Script Ä‘á»ƒ restart training tá»« checkpoint vá»›i learning rate tháº¥p hÆ¡n
DÃ¹ng khi gáº·p váº¥n Ä‘á» SSIM collapse hoáº·c training instability
"""

import os
import torch
import glob
from src.train import CycleGANTrainer
from src.volume_based_cached_loader import VolumeCachedDataLoaderManager

def find_latest_checkpoint(checkpoint_dir):
    """TÃ¬m checkpoint má»›i nháº¥t"""
    pattern = os.path.join(checkpoint_dir, "checkpoint_epoch_*.pth")
    checkpoint_files = glob.glob(pattern)
    
    if not checkpoint_files:
        return None
    
    # Sáº¯p xáº¿p theo epoch number
    def extract_epoch(filename):
        return int(filename.split('_epoch_')[1].split('.pth')[0])
    
    latest_checkpoint = max(checkpoint_files, key=extract_epoch)
    return latest_checkpoint, extract_epoch(latest_checkpoint)

def restart_training_with_lower_lr():
    """
    Restart training vá»›i learning rate tháº¥p hÆ¡n Ä‘á»ƒ phá»¥c há»“i tá»« SSIM collapse
    """
    
    print("ğŸ”„ RESTART TRAINING WITH LOWER LEARNING RATE")
    print("=" * 60)
    
    # Cáº¥u hÃ¬nh vá»›i learning rate Ráº¤T THáº¤P Ä‘á»ƒ phá»¥c há»“i
    config = {
        # Data parameters  
        'cache_dir': '../preprocessed_cache',
        'batch_size': 2,          # Giáº£m batch size xuá»‘ng 2 Ä‘á»ƒ á»•n Ä‘á»‹nh hÆ¡n
        'num_workers': 2,
        'train_split': 0.8,
        'augmentation_prob': 0.5, # Giáº£m augmentation Ä‘á»ƒ á»•n Ä‘á»‹nh
        
        # Model parameters
        'input_nc': 1,
        'output_nc': 1,
        'n_residual_blocks': 9,   
        'discriminator_layers': 3,
        
        # Training parameters - LEARNING RATE Ráº¤T THáº¤P
        'num_epochs': 300,        # TÄƒng epochs vÃ¬ LR tháº¥p
        'lr_G': 0.00001,         # Giáº£m 10x: 0.0001 -> 0.00001
        'lr_D': 0.00001,         # Giáº£m 10x: 0.0001 -> 0.00001
        'beta1': 0.5,
        'beta2': 0.999,
        'decay_epoch': 100,
        'decay_epochs': 100,
        
        # Directories
        'checkpoint_dir': 'checkpoints',
        'log_dir': 'logs_restart',  # Log riÃªng Ä‘á»ƒ khÃ´ng bá»‹ conflict
        'sample_dir': 'samples_restart',
        
        # Save frequencies
        'save_freq': 2,           # Save thÆ°á»ng xuyÃªn hÆ¡n
        'sample_freq': 5
    }
    
    # Táº¡o thÆ° má»¥c
    for dir_name in ['checkpoint_dir', 'log_dir', 'sample_dir']:
        os.makedirs(config[dir_name], exist_ok=True)
    
    # TÃ¬m checkpoint gáº§n nháº¥t
    latest_checkpoint, latest_epoch = find_latest_checkpoint(config['checkpoint_dir'])
    
    if not latest_checkpoint:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y checkpoint nÃ o Ä‘á»ƒ restart!")
        print("   Cháº¡y training bÃ¬nh thÆ°á»ng trÆ°á»›c.")
        return
    
    print(f"ğŸ“‚ Found latest checkpoint: {latest_checkpoint}")
    print(f"ğŸ“Š Latest epoch: {latest_epoch}")
    print(f"ğŸ”» New learning rates: G={config['lr_G']:.6f}, D={config['lr_D']:.6f}")
    
    # XÃ¡c nháº­n tá»« user
    confirm = input(f"\nâ“ Restart training tá»« epoch {latest_epoch} vá»›i LR tháº¥p hÆ¡n? (y/n): ").lower().strip()
    if confirm not in ['y', 'yes', '']:
        print("âŒ Há»§y restart training.")
        return
    
    # Kiá»ƒm tra GPU
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ Sá»­ dá»¥ng device: {device}")
    
    # Táº¡o data loaders
    print("ğŸ“¦ Äang táº¡o data loaders...")
    loader_manager = VolumeCachedDataLoaderManager(config['cache_dir'])
    
    train_loader, val_loader = loader_manager.create_train_val_loaders(
        batch_size=config['batch_size'],
        train_split=config['train_split'],
        num_workers=config['num_workers'],
        augmentation_prob=config['augmentation_prob']
    )
    
    print(f"âœ… Train batches: {len(train_loader)}, Val batches: {len(val_loader)}")
    
    # Táº¡o trainer vÃ  load checkpoint
    print("ğŸ—ï¸  Äang khá»Ÿi táº¡o trainer...")
    trainer = CycleGANTrainer(
        config=config,
        device=device,
        resume_from_checkpoint=True  # Sáº½ tá»± Ä‘á»™ng load checkpoint
    )
    
    # Thay Ä‘á»•i learning rate sau khi load checkpoint
    print("ğŸ”„ Äang thay Ä‘á»•i learning rate...")
    
    # Update learning rate cho cáº£ optimizer vÃ  scheduler
    for param_group in trainer.optimizer_G.param_groups:
        param_group['lr'] = config['lr_G']
    for param_group in trainer.optimizer_D.param_groups:
        param_group['lr'] = config['lr_D']
    
    # Reset schedulers vá»›i learning rate má»›i
    trainer.scheduler_G = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        trainer.optimizer_G, 
        T_0=30,      # Cycle dÃ i hÆ¡n vÃ¬ LR tháº¥p
        T_mult=2,
        eta_min=config['lr_G'] * 0.1  # Min LR = 10% cá»§a LR má»›i
    )
    
    trainer.scheduler_D = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        trainer.optimizer_D,
        T_0=30,
        T_mult=2, 
        eta_min=config['lr_D'] * 0.1
    )
    
    # Reset early stopping counter
    trainer.epochs_without_improvement = 0
    trainer.max_patience = 20  # TÄƒng patience vÃ¬ LR tháº¥p
    
    current_lr = trainer.get_current_lr()
    print(f"âœ… Learning rates updated: G={current_lr['lr_G']:.6f}, D={current_lr['lr_D']:.6f}")
    
    print("\nğŸš€ Báº®T Äáº¦U RESTART TRAINING...")
    print(f"ğŸ“Š Current epoch: {trainer.current_epoch}")
    print(f"ğŸ† Best SSIM so far: {trainer.best_ssim:.4f}")
    print(f"â±ï¸  Max patience: {trainer.max_patience}")
    print("=" * 60)
    
    # Báº¯t Ä‘áº§u training
    try:
        trainer.train(train_loader, val_loader)
    except KeyboardInterrupt:
        print("\nâš ï¸  Training interrupted by user")
    except Exception as e:
        print(f"\nâŒ Training error: {e}")
        raise
    finally:
        print("\nğŸ”š Training session ended")

if __name__ == "__main__":
    restart_training_with_lower_lr() 